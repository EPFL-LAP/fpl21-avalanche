diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/libs/libarchfpga/src/physical_types.h vtr8_avalanche/libs/libarchfpga/src/physical_types.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/libs/libarchfpga/src/physical_types.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/libs/libarchfpga/src/physical_types.h	2021-08-11 10:31:49.848816000 +0200
@@ -1199,6 +1199,9 @@
     enum e_directionality directionality;
     std::vector<bool> cb;
     std::vector<bool> sb;
+    int prev_usage; //SN: Used to calculate the differential term of the avalanche cost reduction.
+    int usage; //SN: Added to track usage of potential edges.
+    int historical_usage; //SN: Historical term of the avalanche costs.
     //float Cmetal_per_m; /* Wire capacitance (per meter) */
 };
 
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/read_options.cpp vtr8_avalanche/vpr/src/base/read_options.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/read_options.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/base/read_options.cpp	2021-08-11 09:16:46.829236000 +0200
@@ -1285,6 +1285,14 @@
         .default_value("min")
         .show_in(argparse::ShowIn::HELP_ONLY);
 
+    place_timing_grp.add_argument(args.import_place_delay_model, "--import_place_delay_model")
+        .help(
+            "Specify a file that holds the placement delay model to be loaded.\n"
+            "Each line holds an integer representing the number of picoseconds delay at the given offset.\n"
+            "Row-major storage of the matrix is assumed.")
+        .default_value("")
+        .show_in(argparse::ShowIn::HELP_ONLY);
+
     place_timing_grp.add_argument(args.place_delay_offset, "--place_delay_offset")
         .help(
             "A constant offset (in seconds) applied to the placer's delay model.")
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/read_options.h vtr8_avalanche/vpr/src/base/read_options.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/read_options.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/base/read_options.h	2021-08-11 09:16:46.849261000 +0200
@@ -94,6 +94,7 @@
     argparse::ArgValue<int> inner_loop_recompute_divider;
     argparse::ArgValue<float> place_exp_first;
     argparse::ArgValue<float> place_exp_last;
+    argparse::ArgValue<std::string> import_place_delay_model;
     argparse::ArgValue<float> place_delay_offset;
     argparse::ArgValue<int> place_delay_ramp_delta_threshold;
     argparse::ArgValue<float> place_delay_ramp_slope;
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/SetupVPR.cpp vtr8_avalanche/vpr/src/base/SetupVPR.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/SetupVPR.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/base/SetupVPR.cpp	2021-08-11 09:16:45.430122000 +0200
@@ -460,6 +460,7 @@
     /* Depends on PlacerOpts->place_algorithm */
     PlacerOpts->enable_timing_computations = Options.ShowPlaceTiming;
 
+    PlacerOpts->import_delay_model = Options.import_place_delay_model;
     PlacerOpts->delay_offset = Options.place_delay_offset;
     PlacerOpts->delay_ramp_delta_threshold = Options.place_delay_ramp_delta_threshold;
     PlacerOpts->delay_ramp_slope = Options.place_delay_ramp_slope;
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/vpr_context.h vtr8_avalanche/vpr/src/base/vpr_context.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/vpr_context.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/base/vpr_context.h	2021-08-12 15:13:30.966636000 +0200
@@ -154,9 +154,30 @@
 
     std::vector<t_rr_switch_inf> rr_switch_inf; /* autogenerated in build_rr_graph based on switch fan-in. [0..(num_rr_switches-1)] */
 
+    int num_arch_segs;
+    t_segment_inf* arch_seg_inf; //SN: Globally stores segment information.
     int num_arch_switches;
     t_arch_switch_inf* arch_switch_inf; /* [0..(num_arch_switches-1)] */
 
+    int has_potential_switches;
+    int zero_total_usage;
+
+    int ripup_all;
+    float avalanche_p;
+    float avalanche_h; 
+    float avalanche_d; 
+    int avalanche_iter_to_zero;
+    float reset_cost;
+
+    float base_cost_scale;
+    float min_base_cost;
+    float max_criticality;
+    float edge_splitter_crit_exp; //Exponent for net criticality when computing the perceived edge-splitter cost.
+    float target_critical_splitter_cost;
+    float M;
+
+    int is_reset_iter; //Tells that we are in the base cost reset regime (first routing iteration).
+
     // Clock Newtworks
     std::vector<std::unique_ptr<ClockNetwork>> clock_networks;
     std::vector<std::unique_ptr<ClockConnection>> clock_connections;
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/vpr_types.h vtr8_avalanche/vpr/src/base/vpr_types.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/base/vpr_types.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/base/vpr_types.h	2021-08-11 09:16:47.325072000 +0200
@@ -807,6 +807,7 @@
     PlaceDelayModelType delay_model_type;
     e_reducer delay_model_reducer;
 
+    std::string import_delay_model;
     float delay_offset;
     int delay_ramp_delta_threshold;
     float delay_ramp_slope;
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/draw/draw.cpp vtr8_avalanche/vpr/src/draw/draw.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/draw/draw.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/draw/draw.cpp	2021-08-12 15:15:11.291841000 +0200
@@ -1771,12 +1771,12 @@
         if (device_ctx.rr_nodes[to_node].direction() != BI_DIRECTION) {
             /* must connect to to_node's wire beginning at x2 */
             if (to_track % 2 == 0) { /* INC wire starts at leftmost edge */
-                VTR_ASSERT(from_xlow < to_xlow);
+                //VTR_ASSERT(from_xlow < to_xlow);
                 x2 = to_chan.left();
                 /* since no U-turns from_track must be INC as well */
                 x1 = draw_coords->tile_x[to_xlow - 1] + draw_coords->get_tile_width();
             } else { /* DEC wire starts at rightmost edge */
-                VTR_ASSERT(from_xhigh > to_xhigh);
+                //VTR_ASSERT(from_xhigh > to_xhigh);
                 x2 = to_chan.right();
                 x1 = draw_coords->tile_x[to_xhigh + 1];
             }
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/place/timing_place_lookup.cpp vtr8_avalanche/vpr/src/place/timing_place_lookup.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/place/timing_place_lookup.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/place/timing_place_lookup.cpp	2021-08-12 15:15:53.368863000 +0200
@@ -81,6 +81,8 @@
                                    t_router_opts router_opts,
                                    bool measure_directconnect);
 
+static vtr::Matrix<float> import_delta_delay_model(const std::string filepath);
+
 static vtr::Matrix<float> compute_delta_delays(const t_placer_opts& palcer_opts, const t_router_opts& router_opts, bool measure_directconnect, size_t longest_length);
 
 float delay_reduce(std::vector<float>& delays, e_reducer reducer);
@@ -122,6 +124,12 @@
                                                            const int num_directs) {
     vtr::ScopedStartFinishTimer timer("Computing placement delta delay look-up");
 
+    if (strcmp(placer_opts.import_delay_model.c_str(), "")) {
+        auto delta_delays = import_delta_delay_model(placer_opts.import_delay_model);
+
+        return std::make_unique<DeltaDelayModel>(std::move(delta_delays), router_opts);
+    }
+
     init_placement_context();
 
     t_chan_width chan_width = setup_chan_width(router_opts, chan_width_dist);
@@ -617,6 +625,32 @@
 
     return delta_delays;
 }
+
+//SN: Loads a delta matrix from a file. All values are assumed to be in ps.
+static vtr::Matrix<float> import_delta_delay_model(const std::string filepath) {
+    vtr::ScopedStartFinishTimer timer("Importing delta delays");
+
+    auto& device_ctx = g_vpr_ctx.device();
+    auto& grid = device_ctx.grid;
+    vtr::Matrix<float> delta_delays({grid.width(), grid.height()});
+
+    int words_read = 0;
+    FILE* f = vtr::fopen(filepath.c_str(), "r");
+    VTR_ASSERT(f != NULL);
+
+    for (size_t dy = 0; dy < delta_delays.dim_size(1); ++dy) {
+        for (size_t dx = 0; dx < delta_delays.dim_size(0); ++dx) {
+            words_read = fscanf(f, "%f", &(delta_delays[dx][dy]));
+            VTR_ASSERT(words_read);
+            if (delta_delays[dx][dy] < 0)
+                delta_delays[dx][dy] = IMPOSSIBLE_DELTA;
+            else
+                delta_delays[dx][dy] *= 1e-12;
+        }
+    }
+
+    return delta_delays;
+}
 
 //Finds a src_rr and sink_rr appropriate for measuring the delay of the current direct specification
 static bool find_direct_connect_sample_locations(const t_direct_inf* direct,
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_common.cpp vtr8_avalanche/vpr/src/route/route_common.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_common.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/route_common.cpp	2021-08-12 15:14:30.619810000 +0200
@@ -245,7 +245,7 @@
 }
 
 bool try_route(int width_fac,
-               const t_router_opts& router_opts,
+               t_router_opts& router_opts,
                const t_analysis_opts& analysis_opts,
                t_det_routing_arch* det_routing_arch,
                std::vector<t_segment_inf>& segment_inf,
@@ -425,10 +425,52 @@
      * ONE MORE net use this routing node.     */
 
     auto& route_ctx = g_vpr_ctx.mutable_routing();
-    auto& device_ctx = g_vpr_ctx.device();
+    auto& device_ctx = g_vpr_ctx.mutable_device();
 
+    auto& node = device_ctx.rr_nodes[inode];
+
+    int prev_occ = route_ctx.rr_node_route_inf[inode].occ();
     int occ = route_ctx.rr_node_route_inf[inode].occ() + add_or_sub;
     route_ctx.rr_node_route_inf[inode].set_occ(occ);
+    if (node.is_edge_splitter())
+    {
+        int cost_index = node.cost_index();
+        int seg_index = device_ctx.rr_indexed_data[cost_index].seg_index;
+
+        route_ctx.rr_node_route_inf[inode].pres_cost = 1.0;
+
+        if ((prev_occ == 0) && (occ > 0))
+        {
+            device_ctx.arch_seg_inf[seg_index].usage += 1;
+        }
+        else if ((prev_occ > 0) && (occ == 0))
+        {
+            device_ctx.arch_seg_inf[seg_index].usage -= 1;
+        }
+
+        VTR_ASSERT(device_ctx.arch_seg_inf[seg_index].usage >= 0);
+        
+        float base_cost = device_ctx.rr_indexed_data[cost_index].saved_base_cost;
+
+        int hist_usage = device_ctx.arch_seg_inf[seg_index].historical_usage;
+        if (device_ctx.is_reset_iter)
+        {
+            device_ctx.rr_indexed_data[cost_index].base_cost = device_ctx.reset_cost; 
+            return;
+        }
+
+        int cur_usage = device_ctx.arch_seg_inf[seg_index].usage;
+        device_ctx.rr_indexed_data[cost_index].base_cost = std::max(0.0f, base_cost
+                                                         - cur_usage * device_ctx.avalanche_p
+                                                         - hist_usage * device_ctx.avalanche_h);
+
+        VTR_ASSERT(cur_usage >= 0);
+        VTR_ASSERT(hist_usage >= 0);
+        VTR_ASSERT(device_ctx.rr_indexed_data[cost_index].base_cost <= 1.1 * base_cost);
+
+        return;
+    }
+
     // can't have negative occupancy
     VTR_ASSERT(occ >= 0);
 
@@ -440,6 +482,37 @@
     }
 }
 
+//SN: This is a debug routine that prints all edge-splitter costs.
+void print_edge_splitter_costs()
+{
+    auto& device_ctx = g_vpr_ctx.device();
+    VTR_LOG("\n\nEdge-splitter costs:\n");
+    for (unsigned i = 0; i < device_ctx.num_arch_segs; ++i)
+    {
+        if (device_ctx.arch_seg_inf[i].length == 0)
+        {
+            float base_cost = device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + i].saved_base_cost;
+            base_cost = std::min(base_cost, device_ctx.rr_indexed_data[CHANX_COST_INDEX_START
+                                + device_ctx.num_arch_segs + i].saved_base_cost);
+            int prev_usage = device_ctx.arch_seg_inf[i].prev_usage;
+            int cur_usage = device_ctx.arch_seg_inf[i].usage;
+            int hist_usage = device_ctx.arch_seg_inf[i].historical_usage;
+            float calc_cost =
+            std::max(0.0f, base_cost - cur_usage * device_ctx.avalanche_p
+                                     - hist_usage * device_ctx.avalanche_h
+                                     - (cur_usage - prev_usage) * device_ctx.avalanche_d);
+
+            float used_base_cost = min(device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + i].base_cost,
+                                       device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + device_ctx.num_arch_segs + i].base_cost);
+
+            VTR_LOG("%d(%s): (%d, %d, %d) -> %g(%g)/%g\n", i, device_ctx.arch_seg_inf[i].name.c_str(),
+                    device_ctx.arch_seg_inf[i].usage, device_ctx.arch_seg_inf[i].historical_usage, prev_usage,
+                    used_base_cost, calc_cost,
+                    device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + i].saved_base_cost);
+        }
+    }
+}
+
 void pathfinder_update_cost(float pres_fac, float acc_fac) {
     /* This routine recomputes the pres_cost and acc_cost of each routing        *
      * resource for the pathfinder algorithm after all nets have been routed.    *
@@ -450,10 +523,71 @@
      * DATE.                                                                     */
 
     int occ, capacity;
-    auto& device_ctx = g_vpr_ctx.device();
+    auto& device_ctx = g_vpr_ctx.mutable_device();
     auto& route_ctx = g_vpr_ctx.mutable_routing();
 
+    if ((device_ctx.avalanche_p < -1e-15) && !device_ctx.is_reset_iter)
+    {
+        int max_usage = -1;
+        float min_base_cost = 1e12;
+        for (int iseg = 0; iseg < device_ctx.num_arch_segs; ++iseg)
+        {
+            int usage = device_ctx.arch_seg_inf[iseg].usage;
+            if (usage > max_usage)
+            {
+                max_usage = usage;
+                min_base_cost = device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + iseg].saved_base_cost;
+            }
+        }
+
+        device_ctx.min_base_cost = min_base_cost;
+        if (max_usage > 0)
+        {
+            device_ctx.avalanche_p = (float)(min_base_cost) / (max_usage * (1.0 + device_ctx.avalanche_iter_to_zero));
+        }
+        else
+        {
+            device_ctx.avalanche_p = 0.0;
+        }
+
+	    if (device_ctx.avalanche_h < -1e-15)
+	        device_ctx.avalanche_h = device_ctx.avalanche_p / 1.0;
+    	if (device_ctx.avalanche_h < -1e-15)
+            device_ctx.avalanche_d = device_ctx.avalanche_p / 10.0;
+    }
+ 
+    VTR_LOG("avalanche_p = %g\n", device_ctx.avalanche_p);
+    VTR_LOG("avalanche_h = %g\n", device_ctx.avalanche_h);
+    VTR_LOG("avalanche_d = %g\n", device_ctx.avalanche_d);
+
+    int total_usage = 0;
     for (size_t inode = 0; inode < device_ctx.rr_nodes.size(); inode++) {
+
+        auto& node = device_ctx.rr_nodes[inode];
+        if (node.is_edge_splitter())
+        {
+            int cost_index = node.cost_index();
+
+            int seg_index = device_ctx.rr_indexed_data[cost_index].seg_index;
+            float base_cost = device_ctx.rr_indexed_data[cost_index].saved_base_cost;
+            int cur_usage = device_ctx.arch_seg_inf[seg_index].usage;
+            total_usage += cur_usage;
+            int hist_usage = device_ctx.arch_seg_inf[seg_index].historical_usage;
+
+            device_ctx.rr_indexed_data[cost_index].base_cost = (device_ctx.is_reset_iter) ? device_ctx.reset_cost :
+                std::max(0.0f, base_cost - cur_usage * device_ctx.avalanche_p
+                                         - hist_usage * device_ctx.avalanche_h);
+           
+            int twin_index = cost_index; 
+            if (cost_index < CHANX_COST_INDEX_START + device_ctx.num_arch_segs)
+                twin_index = cost_index + device_ctx.num_arch_segs;
+            else
+                twin_index = cost_index - device_ctx.num_arch_segs;
+
+            device_ctx.rr_indexed_data[twin_index].base_cost = device_ctx.rr_indexed_data[cost_index].base_cost;
+
+            continue;
+        }
         occ = route_ctx.rr_node_route_inf[inode].occ();
         capacity = device_ctx.rr_nodes[inode].capacity();
 
@@ -462,13 +596,23 @@
             route_ctx.rr_node_route_inf[inode].pres_cost = 1.0 + (occ + 1 - capacity) * pres_fac;
         }
 
-        /* If occ == capacity, we don't need to increase acc_cost, but a change    *
-         * in pres_fac could have made it necessary to recompute the cost anyway.  */
+        //SN NOTE: It does not matter if the pres_cost is > 1 for edge-splitters, since
+        //their total cost is always returned as the base cost.
 
         else if (occ == capacity) {
             route_ctx.rr_node_route_inf[inode].pres_cost = 1.0 + pres_fac;
         }
     }
+    device_ctx.zero_total_usage = 0;
+    if (total_usage == 0)
+        device_ctx.zero_total_usage = 1;
+
+    print_edge_splitter_costs();
+    for (int iseg = 0; iseg < device_ctx.num_arch_segs; ++iseg)
+    {
+        device_ctx.arch_seg_inf[iseg].prev_usage = device_ctx.arch_seg_inf[iseg].usage;
+        device_ctx.arch_seg_inf[iseg].historical_usage += device_ctx.arch_seg_inf[iseg].usage;
+    }
 }
 
 void init_heap(const DeviceGrid& grid) {
@@ -743,13 +887,17 @@
 /* Returns the congestion cost of using this rr_node, *ignoring* 
  * non-configurable edges */
 static float get_single_rr_cong_cost(int inode) {
-    auto& device_ctx = g_vpr_ctx.device();
+    auto& device_ctx = g_vpr_ctx.mutable_device();
     auto& route_ctx = g_vpr_ctx.routing();
 
     auto cost_index = device_ctx.rr_nodes[inode].cost_index();
+    if (device_ctx.rr_nodes[inode].is_edge_splitter())
+        return device_ctx.rr_indexed_data[cost_index].base_cost;
+
     float cost = device_ctx.rr_indexed_data[cost_index].base_cost
                  * route_ctx.rr_node_route_inf[inode].acc_cost
                  * route_ctx.rr_node_route_inf[inode].pres_cost;
+
     return cost;
 }
 
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_common.h vtr8_avalanche/vpr/src/route/route_common.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_common.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/route_common.h	2021-08-11 09:16:50.044918000 +0200
@@ -32,6 +32,9 @@
  *          linked list.  Not used when on the heap.
  *
  */
+
+void print_edge_splitter_costs();
+
 struct t_heap {
     float cost = 0.;
     float backward_path_cost = 0.;
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_export.h vtr8_avalanche/vpr/src/route/route_export.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_export.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/route_export.h	2021-08-11 09:16:50.060949000 +0200
@@ -9,7 +9,7 @@
 void try_graph(int width_fac, t_router_opts router_opts, t_det_routing_arch* det_routing_arch, std::vector<t_segment_inf>& segment_inf, t_chan_width_dist chan_width_dist, t_direct_inf* directs, int num_directs);
 
 bool try_route(int width_fac,
-               const t_router_opts& router_opts,
+               t_router_opts& router_opts,
                const t_analysis_opts& analysis_opts,
                t_det_routing_arch* det_routing_arch,
                std::vector<t_segment_inf>& segment_inf,
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/router_delay_profiling.cpp vtr8_avalanche/vpr/src/route/router_delay_profiling.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/router_delay_profiling.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/router_delay_profiling.cpp	2021-08-11 09:16:50.325898000 +0200
@@ -41,6 +41,7 @@
     std::vector<int> modified_rr_node_inf;
     RouterStats router_stats;
     auto router_lookahead = make_router_lookahead(router_opts.lookahead_type);
+    //printf("Delay profiling lookahead computed.\n");
     t_heap* cheapest = timing_driven_route_connection_from_route_tree(rt_root, sink_node, cost_params, bounding_box, *router_lookahead, modified_rr_node_inf, router_stats);
 
     bool found_path = (cheapest != nullptr);
@@ -48,6 +49,7 @@
         VTR_ASSERT(cheapest->index == sink_node);
 
         t_rt_node* rt_node_of_sink = update_route_tree(cheapest, nullptr);
+        //SN_DEBUG: print_route_tree(rt_root);
         free_heap_data(cheapest);
 
         //find delay
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/router_lookahead_map.cpp vtr8_avalanche/vpr/src/route/router_lookahead_map.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/router_lookahead_map.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/router_lookahead_map.cpp	2021-08-12 14:46:46.546011000 +0200
@@ -163,6 +163,12 @@
             VTR_ASSERT(device_ctx.rr_indexed_data[cost_index].C_load <= 0.);
             this->delay += device_ctx.rr_indexed_data[cost_index].T_linear;
 
+            if (device_ctx.rr_indexed_data[cost_index].base_cost > 1e-8)
+            {
+                printf("Cost not reset for %d (%g)\n", cost_index, device_ctx.rr_indexed_data[cost_index].base_cost);
+                char tmp;
+                scanf("%c", &tmp);
+            }
             this->congestion_upstream += device_ctx.rr_indexed_data[cost_index].base_cost;
         }
 
@@ -237,26 +243,52 @@
     Cost_Entry cost_entry = f_cost_map[from_chan_index][from_seg_index][delta_x][delta_y];
     float expected_delay = cost_entry.delay;
     float expected_congestion = cost_entry.congestion;
-
     float expected_cost = criticality_fac * expected_delay + (1.0 - criticality_fac) * expected_congestion;
     return expected_cost;
 }
 
 /* Computes the lookahead map to be used by the router. If a map was computed prior to this, a new one will not be computed again.
  * The rr graph must have been built before calling this function. */
-void compute_router_lookahead(int num_segments) {
+void compute_router_lookahead(int num_segments, std::vector<t_segment_inf>& segment_inf) {
     vtr::ScopedStartFinishTimer timer("Computing router lookahead map");
 
     f_cost_map.clear();
 
-    auto& device_ctx = g_vpr_ctx.device();
-
+    auto& device_ctx = g_vpr_ctx.mutable_device();
+    device_ctx.ripup_all = 0;
+        
     /* free previous delay map and allocate new one */
     free_cost_map();
     alloc_cost_map(num_segments);
 
+    //SN: First reset the base costs of the edge-splitters to zero, to keep lookahead admissability as the costs drop.
+    for (int iseg = 0; iseg < num_segments; iseg++)
+    {
+        int length = segment_inf[iseg].length;
+        if (length != 0)
+        {
+            continue;
+        }
+   
+         device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + iseg].base_cost = 0;
+         device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + num_segments + iseg].base_cost = 0;
+ 
+         //SN: Also set the segment usage fields to zero.
+        segment_inf[iseg].prev_usage = 0;
+        segment_inf[iseg].usage = 0;
+        segment_inf[iseg].historical_usage = 0;
+        device_ctx.arch_seg_inf[iseg].prev_usage = 0;
+        device_ctx.arch_seg_inf[iseg].historical_usage = 0;
+        device_ctx.arch_seg_inf[iseg].usage = 0;
+    }
+
     /* run Dijkstra's algorithm for each segment type & channel type combination */
     for (int iseg = 0; iseg < num_segments; iseg++) {
+        int length = segment_inf[iseg].length;
+        if (length == 0)
+        {
+            continue;
+        }
         for (e_rr_type chan_type : {CHANX, CHANY}) {
             /* allocate the cost map for this iseg/chan_type */
             t_routing_cost_map routing_cost_map({device_ctx.grid.width(), device_ctx.grid.height()});
@@ -287,6 +319,60 @@
         }
     }
 
+    //SN: Now copy the computed map to the edge-splitting nodes:
+    for (int iseg = 0; iseg < num_segments; iseg++)
+    {
+        int length = segment_inf[iseg].length;
+        if (length != 0)
+            continue;
+
+        //SN: Now restore the base cost.
+        device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + iseg].base_cost
+        = device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + iseg].saved_base_cost;   
+
+        device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + num_segments + iseg].base_cost
+        = device_ctx.rr_indexed_data[CHANX_COST_INDEX_START + num_segments + iseg].saved_base_cost;   
+
+        int center_x = device_ctx.grid.width() / 2;
+        int center_y = device_ctx.grid.height() / 2;
+        e_rr_type chan_type = CHANX; 
+        int start_node_ind = get_start_node_ind(center_y, center_x,
+                                                device_ctx.grid.width() - 2, device_ctx.grid.height() - 2,
+                                                chan_type, iseg, -1);
+        //NOTE: CHANX coordinates are inverted in device_ctx.rr_node_indices, for some reason (see l.801 in rr_graph_reader.cpp)
+        if (start_node_ind == UNDEFINED)
+        {
+            chan_type = CHANY;
+            start_node_ind = get_start_node_ind(center_x, center_y,
+                                                device_ctx.grid.width() - 2, device_ctx.grid.height() - 2,
+                                                chan_type, iseg, -1);
+            VTR_ASSERT(start_node_ind != UNDEFINED);
+        }
+
+        auto& start_node = device_ctx.rr_nodes[start_node_ind];
+        VTR_ASSERT(start_node.num_edges() > 0);
+
+        int child_ind = start_node.edge_sink_node(0);
+        auto& child_node = device_ctx.rr_nodes[child_ind];
+        
+        int child_cost_index = child_node.cost_index();
+        int child_seg_index = device_ctx.rr_indexed_data[child_cost_index].seg_index;
+
+        VTR_ASSERT(child_seg_index >= 0);
+
+        for (unsigned chan_index = 0; chan_index < 2; ++chan_index)
+        {
+            for (unsigned ix = 0; ix < device_ctx.grid.width(); ix++)
+            {
+                for (unsigned iy = 0; iy < device_ctx.grid.height(); iy++)
+                {
+                    Cost_Entry cost_entry = f_cost_map[chan_index][child_seg_index][ix][iy];
+                    f_cost_map[chan_index][iseg][ix][iy] = cost_entry;
+                }
+            }
+        }
+    }
+
     if (false) print_cost_map();
 }
 
@@ -313,15 +399,18 @@
     /* find first node in channel that has specified segment index and goes in the desired direction */
     for (unsigned itrack = 0; itrack < channel_node_list.size(); itrack++) {
         int node_ind = channel_node_list[itrack];
+        if (node_ind < 0)
+            continue;
+        //SN TODO: Check if this is the right thing to do.
 
         e_direction node_direction = device_ctx.rr_nodes[node_ind].direction();
         int node_cost_ind = device_ctx.rr_nodes[node_ind].cost_index();
         int node_seg_ind = device_ctx.rr_indexed_data[node_cost_ind].seg_index;
 
-        if ((node_direction == direction || node_direction == BI_DIRECTION) && node_seg_ind == seg_index) {
+        if ((track_offset < 0 || node_direction == direction || node_direction == BI_DIRECTION) && node_seg_ind == seg_index) {
             /* found first track that has the specified segment index and goes in the desired direction */
             result = node_ind;
-            if (track_offset == 0) {
+            if (track_offset <= 0) {
                 break;
             }
             track_offset -= 2;
@@ -454,7 +543,6 @@
     for (unsigned ix = 0; ix < device_ctx.grid.width(); ix++) {
         for (unsigned iy = 0; iy < device_ctx.grid.height(); iy++) {
             Cost_Entry cost_entry = f_cost_map[chan_index][segment_index][ix][iy];
-
             if (cost_entry.delay < 0 && cost_entry.congestion < 0) {
                 Cost_Entry copied_entry = get_nearby_cost_entry(ix, iy, segment_index, chan_index);
                 f_cost_map[chan_index][segment_index][ix][iy] = copied_entry;
@@ -463,9 +551,11 @@
     }
 }
 
-/* returns a cost entry in the f_cost_map that is near the specified coordinates (and preferably towards (0,0)) */
+
+//SN: Bug fix taken from the official development branch of VPR,  "Latest commit f51482e on Sep 14"
+/* returns a cost entry in the f_wire_cost_map that is near the specified coordinates (and preferably towards (0,0)) */
 static Cost_Entry get_nearby_cost_entry(int x, int y, int segment_index, int chan_index) {
-    /* compute the slope from x,y to 0,0 and then move towards 0,0 by one unit to get the coordinates
+    /* , segment_infcompute the slope from x,y to 0,0 and then move towards 0,0 by one unit to get the coordinates
      * of the cost entry to be copied */
 
     //VTR_ASSERT(x > 0 || y > 0); //Asertion fails in practise. TODO: debug
@@ -488,13 +578,18 @@
         copy_y = vtr::nint((float)x * slope);
     }
 
-    //VTR_ASSERT(copy_x > 0 || copy_y > 0); //Asertion fails in practise. TODO: debug
+    copy_y = std::max(copy_y, 0); //Clip to zero
+    copy_x = std::max(copy_x, 0); //Clip to zero
 
     Cost_Entry copy_entry = f_cost_map[chan_index][segment_index][copy_x][copy_y];
 
     /* if the entry to be copied is also empty, recurse */
     if (copy_entry.delay < 0 && copy_entry.congestion < 0) {
-        copy_entry = get_nearby_cost_entry(copy_x, copy_y, segment_index, chan_index);
+        if (copy_x == 0 && copy_y == 0) {
+            copy_entry = Cost_Entry(0., 0.); //(0, 0) entry is invalid so set zero to terminate recursion
+        } else {
+            copy_entry = get_nearby_cost_entry(copy_x, copy_y, segment_index, chan_index);
+        }
     }
 
     return copy_entry;
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/router_lookahead_map.h vtr8_avalanche/vpr/src/route/router_lookahead_map.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/router_lookahead_map.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/router_lookahead_map.h	2021-08-11 09:16:50.413915000 +0200
@@ -2,7 +2,7 @@
 
 /* Computes the lookahead map to be used by the router. If a map was computed prior to this, a new one will not be computed again.
  * The rr graph must have been built before calling this function. */
-void compute_router_lookahead(int num_segments);
+void compute_router_lookahead(int num_segments, std::vector<t_segment_inf>& segment_inf);
 
 /* queries the lookahead_map (should have been computed prior to routing) to get the expected cost
  * from the specified source to the specified target */
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_timing.cpp vtr8_avalanche/vpr/src/route/route_timing.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_timing.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/route_timing.cpp	2021-08-12 15:07:26.923839000 +0200
@@ -303,7 +303,7 @@
 static bool same_non_config_node_set(int from_node, int to_node);
 
 /************************ Subroutine definitions *****************************/
-bool try_timing_driven_route(const t_router_opts& router_opts,
+bool try_timing_driven_route(t_router_opts& router_opts,
                              const t_analysis_opts& analysis_opts,
                              vtr::vector<ClusterNetId, float*>& net_delay,
                              const ClusteredPinAtomPinsLookup& netlist_pin_lookup,
@@ -316,6 +316,7 @@
 
     auto& cluster_ctx = g_vpr_ctx.clustering();
     auto& route_ctx = g_vpr_ctx.mutable_routing();
+    auto& device_ctx = g_vpr_ctx.mutable_device();
 
     //Initially, the router runs normally trying to reduce congestion while
     //balancing other metrics (timing, wirelength, run-time etc.)
@@ -360,11 +361,13 @@
     route_budgets budgeting_inf;
 
     auto router_lookahead = make_router_lookahead(router_opts.lookahead_type);
+    printf("Lookahead computed.\n");
 
     /*
      * Routing parameters
      */
     float pres_fac = router_opts.first_iter_pres_fac; /* Typically 0 -> ignore cong. */
+    device_ctx.is_reset_iter = 1;
     int bb_fac = router_opts.bb_factor;
 
     //When routing conflicts are detected the bounding boxes are scaled
@@ -405,6 +408,21 @@
     int num_net_bounding_boxes_updated = 0;
     int itry_since_last_convergence = -1;
     for (itry = 1; itry <= router_opts.max_router_iterations; ++itry) {
+
+        const double RIPUP_INTERVAL = 5.0;
+        if ((itry % (int)(std::min(1.0 + 0.1 * (legal_convergence_count ? itry_since_last_convergence : itry), RIPUP_INTERVAL))) == 0)
+        {
+            device_ctx.ripup_all = 1;
+        }
+        else
+        {
+            device_ctx.ripup_all = 0;
+        }
+        if ((device_ctx.has_potential_switches == 0) || (device_ctx.zero_total_usage))
+        {
+            device_ctx.ripup_all = 0;
+        }
+
         RouterStats router_iteration_stats;
 
         /* Reset "is_routed" and "is_fixed" flags to indicate nets not pre-routed (yet) */
@@ -434,7 +452,13 @@
         /*
          * Route each net
          */
+        
+        if (itry == 1)
+        {
+            pathfinder_update_cost(pres_fac, 0.); /* Acc_fac=0 for first iter. */
+        }
         for (auto net_id : sorted_nets) {
+            //VTR_LOG("Routing net %d\n", net_id);
             bool is_routable = try_timing_driven_route_net(net_id,
                                                            itry,
                                                            pres_fac,
@@ -532,12 +556,12 @@
                     best_routing_metrics.critical_path = critical_path;
                 }
                 best_routing_metrics.used_wirelength = wirelength_info.used_wirelength();
+                router_opts.reconvergence_cpd_threshold = 2.0;
             }
-
             //Decrease pres_fac so that criticl connections will take more direct routes
             //Note that we use first_iter_pres_fac here (typically zero), and switch to
             //use initial_pres_fac on the next iteration.
-            pres_fac = router_opts.first_iter_pres_fac;
+            //pres_fac = router_opts.first_iter_pres_fac;
 
             //Reduce timing tolerances to re-route more delay-suboptimal signals
             connections_inf.set_connection_criticality_tolerance(0.7);
@@ -554,6 +578,8 @@
             //after the first routing convergence. Since that is often zero,
             //we want to set pres_fac to a reasonable (i.e. typically non-zero)
             //value afterwards -- so it grows when multiplied by pres_fac_mult
+
+            //SN: We do not wish to lose the congestion information when trying to reconcentrate the low-utilized nodes.
             pres_fac = router_opts.initial_pres_fac;
         }
 
@@ -601,13 +627,12 @@
         //Update pres_fac and resource costs
         if (itry == 1) {
             pres_fac = router_opts.initial_pres_fac;
+            device_ctx.is_reset_iter = 0;
             pathfinder_update_cost(pres_fac, 0.); /* Acc_fac=0 for first iter. */
         } else {
             pres_fac *= router_opts.pres_fac_mult;
-
             /* Avoid overflow for high iteration counts, even if acc_cost is big */
             pres_fac = min(pres_fac, static_cast<float>(HUGE_POSITIVE_FLOAT / 1e5));
-
             pathfinder_update_cost(pres_fac, router_opts.acc_fac);
         }
 
@@ -674,7 +699,6 @@
 
                 //Yes, if explicitly enabled
                 bool should_ripup_for_delay = (router_opts.incr_reroute_delay_ripup == e_incr_reroute_delay_ripup::ON);
-
                 //Or, if things are not too congested
                 should_ripup_for_delay |= (router_opts.incr_reroute_delay_ripup == e_incr_reroute_delay_ripup::AUTO
                                            && router_congestion_mode == RouterCongestionMode::NORMAL);
@@ -717,17 +741,21 @@
         VTR_LOG("Restoring best routing\n");
 
         auto& router_ctx = g_vpr_ctx.mutable_routing();
+        print_edge_splitter_costs();
 
         /* Restore congestion from best route */
         for (auto net_id : cluster_ctx.clb_nlist.nets()) {
             pathfinder_update_path_cost(route_ctx.trace[net_id].head, -1, pres_fac);
             pathfinder_update_path_cost(best_routing[net_id].head, 1, pres_fac);
         }
+
         router_ctx.trace = best_routing;
         router_ctx.clb_opins_used_locally = best_clb_opins_used_locally;
 
+        //print_edge_splitter_costs();
         prune_unused_non_configurable_nets(connections_inf);
 
+        //print_edge_splitter_costs();
         if (timing_info) {
             VTR_LOG("Critical path: %g ns\n", 1e9 * best_routing_metrics.critical_path.delay());
         }
@@ -736,7 +764,7 @@
     } else {
         VTR_LOG("Routing failed.\n");
 #ifdef VTR_ENABLE_DEBUG_LOGGING
-        if (f_router_debug) print_invalid_routing_info();
+        if (1 || f_router_debug) print_invalid_routing_info();
 #endif
     }
 
@@ -762,6 +790,11 @@
     auto& cluster_ctx = g_vpr_ctx.clustering();
     auto& route_ctx = g_vpr_ctx.mutable_routing();
 
+    auto& device_ctx = g_vpr_ctx.mutable_device();
+    device_ctx.max_criticality = router_opts.max_criticality;
+    device_ctx.M = log(device_ctx.target_critical_splitter_cost / device_ctx.base_cost_scale)
+                 / pow(device_ctx.max_criticality, device_ctx.edge_splitter_crit_exp);
+   
     bool is_routed = false;
 
     connections_inf.prepare_routing_for_net(net_id);
@@ -986,6 +1019,8 @@
         enable_router_debug(router_opts, net_id, sink_rr);
 
         cost_params.criticality = pin_criticality[target_pin];
+        cost_params.exp_criticality = exp(device_ctx.M * pow(cost_params.criticality, device_ctx.edge_splitter_crit_exp)); 
+        //NOTE: We switched to exponential drop because it discriminates the critical net in a more gradual manner in the region where it is of interest.
 
         if (budgeting_inf.if_set()) {
             conn_delay_budget.max_delay = budgeting_inf.get_max_delay_budget(net_id, target_pin);
@@ -1825,6 +1860,16 @@
     int to_xhigh = device_ctx.rr_nodes[to_node].xhigh();
     int to_yhigh = device_ctx.rr_nodes[to_node].yhigh();
 
+    auto& node = device_ctx.rr_nodes[to_node];
+    if (node.is_edge_splitter())
+    {
+        int child_ind = node.edge_sink_node(0);
+        to_xlow = device_ctx.rr_nodes[child_ind].xlow();
+        to_ylow = device_ctx.rr_nodes[child_ind].ylow();
+        to_xhigh = device_ctx.rr_nodes[child_ind].xhigh();
+        to_yhigh = device_ctx.rr_nodes[child_ind].yhigh();
+    }
+
     if (to_xhigh < bounding_box.xmin      //Strictly left of BB left-edge
         || to_xlow > bounding_box.xmax    //Strictly right of BB right-edge
         || to_yhigh < bounding_box.ymin   //Strictly below BB bottom-edge
@@ -1978,8 +2023,61 @@
     }
 
     //Update the backward cost (upstream already included)
-    to->backward_path_cost += (1. - cost_params.criticality) * cong_cost; //Congestion cost
-    to->backward_path_cost += cost_params.criticality * Tdel;             //Delay cost
+
+    auto& to_node_node = device_ctx.rr_nodes[to_node];
+
+    float total_cost_edge_split_inc = 0.0;
+    if (to_node_node.is_edge_splitter())
+    {
+        if (device_ctx.max_criticality <= 1e-3)
+        {
+            to->backward_path_cost += cong_cost;
+        }
+        else
+        {
+            to->backward_path_cost += cost_params.exp_criticality * cong_cost;
+        }
+        to->backward_path_cost += Tdel;
+
+        const int include_edge_split_lookahead = 1;
+        if(include_edge_split_lookahead)
+        {
+            int child_ind = to_node_node.edge_sink_node(0);
+            auto& child_node = device_ctx.rr_nodes[child_ind];
+            int child_cost_ind = child_node.cost_index();
+            float child_cong_cost = get_rr_cong_cost(child_ind); 
+            if (device_ctx.max_criticality <= 1e-3)
+            {
+                total_cost_edge_split_inc += 2 * (1. - cost_params.criticality) * child_cong_cost;
+            }
+            else
+            {
+                total_cost_edge_split_inc += 2 * cost_params.exp_criticality * child_cong_cost;
+
+                //We multiply by 2 because the congestion on the edge-splitter is the same as on its child.
+                //The base cost will tend to 0, which will discourage congestion removal, so including its cost directly
+                //would not have the right effect. Remember: we want concentration on types, but not on instances.
+
+                int child_switch = device_ctx.rr_nodes[to_node].edge_switch(0);
+                float child_switch_Tdel = device_ctx.rr_switch_inf[child_switch].Tdel;
+                total_cost_edge_split_inc += cost_params.criticality * child_switch_Tdel;
+            }
+        }
+        //Lookahead starts to make sense only when the congestion cost rises a lot, by which time we are almost done in any case.
+        //It takes a lot of time to compute (1/5 of the total runtime, for the first iteration on (alu4, tseng, ex5p wafer).
+    }
+    else 
+    {
+        if (device_ctx.max_criticality <= 1e-3)
+        {
+            to->backward_path_cost += (1. - cost_params.criticality) * cong_cost; //Congestion cost
+        }
+        else
+        {
+            to->backward_path_cost += cost_params.exp_criticality * cong_cost; //Congestion cost
+        }
+        to->backward_path_cost += cost_params.criticality * Tdel;             //Delay cost
+    }
 
     if (cost_params.bend_cost != 0.) {
         t_rr_type from_type = device_ctx.rr_nodes[from_node].type();
@@ -2005,7 +2103,8 @@
 
     //Update total cost
     float expected_cost = router_lookahead.get_expected_cost(to_node, target_node, cost_params, to->R_upstream);
-    total_cost = to->backward_path_cost + cost_params.astar_fac * expected_cost;
+    total_cost = to->backward_path_cost + cost_params.astar_fac * expected_cost + total_cost_edge_split_inc;
+    //Because astar_fac is > 1.0, edge_splitter lookahead inc should go without it, because it is tight.
 
     to->cost = total_cost;
 }
@@ -2022,6 +2121,9 @@
     factor = sqrt(fanout);
 
     for (index = CHANX_COST_INDEX_START; index < device_ctx.rr_indexed_data.size(); index++) {
+        int seg_index = device_ctx.rr_indexed_data[index].seg_index;
+        if (device_ctx.arch_seg_inf[seg_index].length == 0)
+            continue;
         if (device_ctx.rr_indexed_data[index].T_quadratic > 0.) { /* pass transistor */
             device_ctx.rr_indexed_data[index].base_cost = device_ctx.rr_indexed_data[index].saved_base_cost * factor;
         } else {
@@ -2075,6 +2177,9 @@
     auto& route_ctx = g_vpr_ctx.routing();
     auto& device_ctx = g_vpr_ctx.device();
 
+    if (device_ctx.ripup_all)
+        return true;
+
     t_trace* tptr = route_ctx.trace[net_id].head;
 
     if (tptr == nullptr) {
@@ -2329,15 +2434,18 @@
     size_t overused_nodes = 0;
     size_t total_overuse = 0;
     size_t worst_overuse = 0;
+    size_t overused_node = 0;
     for (size_t inode = 0; inode < device_ctx.rr_nodes.size(); inode++) {
         int overuse = route_ctx.rr_node_route_inf[inode].occ() - device_ctx.rr_nodes[inode].capacity();
         if (overuse > 0) {
             overused_nodes += 1;
+            overused_node = inode;
 
             total_overuse += overuse;
             worst_overuse = std::max(worst_overuse, size_t(overuse));
         }
     }
+    printf("Overused RR-node: %d\n", overused_node);
     return OveruseInfo(device_ctx.rr_nodes.size(), overused_nodes, total_overuse, worst_overuse);
 }
 
@@ -2708,6 +2816,9 @@
                                                int itry_since_last_convergence,
                                                std::shared_ptr<const SetupHoldTimingInfo> timing_info,
                                                const RoutingMetrics& best_routing_metrics) {
+
+    return false; //SN: FIXME: You actually want to check nonzero switch utilization convergence (or some epsilon).
+
     //Give-up on reconvergent routing if the CPD improvement after the
     //first iteration since convergence is small, compared to the best
     //CPD seen so far
Binary files vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/.route_timing.cpp.swp and vtr8_avalanche/vpr/src/route/.route_timing.cpp.swp differ
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_timing.h vtr8_avalanche/vpr/src/route/route_timing.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_timing.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/route_timing.h	2021-08-11 09:16:50.163913000 +0200
@@ -13,7 +13,7 @@
 
 int get_max_pins_per_net();
 
-bool try_timing_driven_route(const t_router_opts& router_opts,
+bool try_timing_driven_route(t_router_opts& router_opts,
                              const t_analysis_opts& analysis_opts,
                              vtr::vector<ClusterNetId, float*>& net_delay,
                              const ClusteredPinAtomPinsLookup& netlist_pin_lookup,
@@ -41,6 +41,7 @@
 
 struct t_conn_cost_params {
     float criticality = 1.;
+    float exp_criticality = 1.;
     float astar_fac = 1.2;
     float bend_cost = 1.;
     const t_conn_delay_budget* delay_budget = nullptr;
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_tree_timing.h vtr8_avalanche/vpr/src/route/route_tree_timing.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/route_tree_timing.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/route_tree_timing.h	2021-08-11 09:16:50.238887000 +0200
@@ -15,7 +15,7 @@
 t_rt_node* init_route_tree_to_source(ClusterNetId inet);
 
 void free_route_tree(t_rt_node* rt_node);
-void print_route_tree(const t_rt_node* rt_node, int depth = 0);
+//void print_route_tree(const t_rt_node* rt_node, int depth = 0);
 
 t_rt_node* update_route_tree(t_heap* hptr, SpatialRouteTreeLookup* spatial_rt_lookup);
 
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_graph.cpp vtr8_avalanche/vpr/src/route/rr_graph.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_graph.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/rr_graph.cpp	2021-08-12 12:11:26.142282000 +0200
@@ -379,7 +379,8 @@
     print_rr_graph_stats();
 
     if (router_lookahead_type == e_router_lookahead::MAP) {
-        compute_router_lookahead(segment_inf.size());
+        printf("Calling map lookahead.\n");
+        compute_router_lookahead(segment_inf.size(), segment_inf);
     }
 
     //Write out rr graph file if needed
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_graph_indexed_data.cpp vtr8_avalanche/vpr/src/route/rr_graph_indexed_data.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_graph_indexed_data.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/rr_graph_indexed_data.cpp	2021-08-12 12:14:01.101444000 +0200
@@ -20,6 +20,8 @@
                                             const t_rr_node_indices& L_rr_node_indices,
                                             enum e_base_cost_type base_cost_type);
 
+static void load_external_base_costs();
+
 static float get_delay_normalization_fac(int nodes_per_chan,
                                          const t_rr_node_indices& L_rr_node_indices);
 
@@ -88,7 +90,11 @@
         else
             length = std::min<int>(segment_inf[iseg].length, device_ctx.grid.width());
 
-        device_ctx.rr_indexed_data[index].inv_length = 1. / length;
+        if (length == 0)
+            device_ctx.rr_indexed_data[index].inv_length = 0.;
+            //SN: Check if it should be set to 0 or some other number, for these zero-length wires.
+        else
+            device_ctx.rr_indexed_data[index].inv_length = 1. / length;
         device_ctx.rr_indexed_data[index].seg_index = iseg;
     }
     load_rr_indexed_data_T_values(CHANX_COST_INDEX_START, num_segment, CHANX,
@@ -109,14 +115,20 @@
         else
             length = std::min<int>(segment_inf[iseg].length, device_ctx.grid.height());
 
-        device_ctx.rr_indexed_data[index].inv_length = 1. / length;
+        if (length == 0)
+            device_ctx.rr_indexed_data[index].inv_length = 0.;
+            //SN: Check if it should be set to 0 or some other number, for these zero-length wires.
+        else
+            device_ctx.rr_indexed_data[index].inv_length = 1. / length;
         device_ctx.rr_indexed_data[index].seg_index = iseg;
     }
     load_rr_indexed_data_T_values((CHANX_COST_INDEX_START + num_segment),
                                   num_segment, CHANY, nodes_per_chan, L_rr_node_indices);
 
+    printf("Loading base costs.\n");
     load_rr_indexed_data_base_costs(nodes_per_chan, L_rr_node_indices,
                                     base_cost_type);
+    printf("All loading done.\n");
 }
 
 void load_rr_index_segments(const int num_segment) {
@@ -176,7 +188,6 @@
 
         } else if (base_cost_type == DELAY_NORMALIZED_LENGTH || base_cost_type == DEMAND_ONLY_NORMALIZED_LENGTH) {
             device_ctx.rr_indexed_data[index].base_cost = delay_normalization_fac / device_ctx.rr_indexed_data[index].inv_length;
-
         } else if (base_cost_type == DELAY_NORMALIZED_FREQUENCY) {
             int seg_index = device_ctx.rr_indexed_data[index].seg_index;
             float freq_fac = float(rr_segment_counts[seg_index]) / total_segments;
@@ -205,6 +216,56 @@
     for (index = 0; index < device_ctx.rr_indexed_data.size(); index++) {
         device_ctx.rr_indexed_data[index].saved_base_cost = device_ctx.rr_indexed_data[index].base_cost;
     }
+   
+    printf("Loading external costs.\n"); 
+    load_external_base_costs();
+    printf("External costs loaded.\n");
+}
+
+//SN: A function to load external base costs, used to initialize the avalanche process.
+static void load_external_base_costs()
+{
+    auto& device_ctx = g_vpr_ctx.mutable_device();
+    FILE* f = vtr::fopen("base_costs.dump", "r");
+    VTR_ASSERT(f != NULL);
+
+    fscanf(f, "%f %f %f %d %f", &(device_ctx.avalanche_p), &(device_ctx.avalanche_h),
+                                &(device_ctx.avalanche_d), &(device_ctx.avalanche_iter_to_zero), &(device_ctx.reset_cost));
+
+    fscanf(f, "%f %f", &(device_ctx.edge_splitter_crit_exp), &(device_ctx.target_critical_splitter_cost));
+
+    unsigned potential_edge_count = -1;
+    int scaling_factor_exponent = 0;
+    float scaling_factor = 0;
+    fscanf(f, "%u %d", &potential_edge_count, &scaling_factor_exponent);
+    scaling_factor = (float)(pow(10, scaling_factor_exponent));
+    device_ctx.base_cost_scale = scaling_factor; 
+
+    device_ctx.target_critical_splitter_cost *= scaling_factor;
+
+    unsigned y_wires_start = device_ctx.num_arch_segs;
+    //We already have the CHANX_START embedded in the external cost indices.
+    
+    for (unsigned i = 0; i < potential_edge_count; ++i)
+    {
+        int ind = -1;
+        float val = 0;
+        fscanf(f, "%d", &ind);
+        fscanf(f, "%f", &val);
+        //printf("%d %g\n", ind, val * scaling_factor);
+        device_ctx.rr_indexed_data[ind].base_cost = val * scaling_factor;
+        device_ctx.rr_indexed_data[ind].saved_base_cost = device_ctx.rr_indexed_data[ind].base_cost;
+        device_ctx.rr_indexed_data[ind + y_wires_start].base_cost = val * scaling_factor;
+        device_ctx.rr_indexed_data[ind + y_wires_start].saved_base_cost = device_ctx.rr_indexed_data[ind].base_cost;
+
+    }
+
+    device_ctx.avalanche_p *= scaling_factor;
+    device_ctx.avalanche_h *= scaling_factor;
+    device_ctx.avalanche_d *= scaling_factor;
+    device_ctx.reset_cost *= scaling_factor;
+ 
+    fclose(f);
 }
 
 static std::vector<size_t> count_rr_segment_types() {
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_graph_reader.cpp vtr8_avalanche/vpr/src/route/rr_graph_reader.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_graph_reader.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/rr_graph_reader.cpp	2021-08-12 12:15:27.545372000 +0200
@@ -93,6 +93,7 @@
         loc_data = pugiutil::load_xml(doc, read_rr_graph_name);
 
         auto& device_ctx = g_vpr_ctx.mutable_device();
+        device_ctx.has_potential_switches = 0;
 
         auto rr_graph = get_single_child(doc, "rr_graph", loc_data);
 
@@ -167,6 +168,16 @@
 
         init_fan_in(device_ctx.rr_nodes, device_ctx.rr_nodes.size());
 
+        //SN: Copy segment info to the global context.
+        printf("Copying segment information to the global context.\n");
+        device_ctx.num_arch_segs = segment_inf.size();
+        device_ctx.arch_seg_inf = new t_segment_inf[segment_inf.size()];
+        for (int iseg = 0; iseg < segment_inf.size(); ++iseg)
+        {
+            device_ctx.arch_seg_inf[iseg] = segment_inf[iseg];
+        }
+
+        printf("Copied. Allocating indexed_data.\n");
         //sets the cost index and seg id information
         next_component = get_single_child(rr_graph, "rr_nodes", loc_data);
         set_cost_indices(next_component, loc_data, is_global_graph, segment_inf.size());
@@ -174,6 +185,7 @@
         alloc_and_load_rr_indexed_data(segment_inf, device_ctx.rr_node_indices,
                                        max_chan_width, *wire_to_rr_ipin_switch, base_cost_type);
 
+        printf("Processing segment ids.\n");
         process_seg_id(next_component, loc_data);
 
         device_ctx.chan_width = nodes_per_chan;
@@ -270,6 +282,16 @@
             if (attribute) {
                 int seg_id = get_attribute(segmentSubnode, "segment_id", loc_data).as_int(0);
                 device_ctx.rr_indexed_data[node.cost_index()].seg_index = seg_id;
+                int length = device_ctx.arch_seg_inf[seg_id].length;
+                if (length == 0)
+                {
+                    node.set_is_edge_splitter(true);
+                    device_ctx.has_potential_switches = 1;
+                }
+                else
+                {
+                    node.set_is_edge_splitter(false);
+                }
             } else {
                 //-1 for non chanx or chany nodes
                 device_ctx.rr_indexed_data[node.cost_index()].seg_index = -1;
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_node.cpp vtr8_avalanche/vpr/src/route/rr_node.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_node.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/rr_node.cpp	2021-08-11 09:16:51.110347000 +0200
@@ -6,6 +6,11 @@
  * resource type string by its index, which is defined by           *
  * "t_rr_type type".												*/
 
+bool t_rr_node::is_edge_splitter() const
+{
+    return is_edge_splitter_;
+}
+
 const char* t_rr_node::type_string() const {
     return rr_node_typename[type()];
 }
@@ -51,11 +56,11 @@
     return ptc_.class_num;
 }
 
-short t_rr_node::cost_index() const {
+int t_rr_node::cost_index() const {
     return cost_index_;
 }
 
-short t_rr_node::rc_index() const {
+int t_rr_node::rc_index() const {
     return rc_index_;
 }
 
@@ -149,6 +154,11 @@
     return true;
 }
 
+void t_rr_node::set_is_edge_splitter(bool is_edge_splitter)
+{
+    is_edge_splitter_ = is_edge_splitter;
+}
+
 void t_rr_node::set_type(t_rr_type new_type) {
     type_ = new_type;
 }
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_node.h vtr8_avalanche/vpr/src/route/rr_node.h
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/rr_node.h	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/rr_node.h	2021-08-11 09:16:51.257333000 +0200
@@ -101,6 +101,8 @@
     short yhigh() const;
     signed short length() const;
 
+    bool is_edge_splitter() const; //SN: Indicates a zero-length edge-splitting node.
+
     short capacity() const;
 
     short ptc_num() const;
@@ -108,8 +110,8 @@
     short track_num() const; //Same as ptc_num() but checks that type() is consistent
     short class_num() const; //Same as ptc_num() but checks that type() is consistent
 
-    short cost_index() const;
-    short rc_index() const;
+    int cost_index() const;
+    int rc_index() const;
     e_direction direction() const;
     const char* direction_string() const;
 
@@ -144,6 +146,8 @@
 
     void set_coordinates(short x1, short y1, short x2, short y2);
 
+    void set_is_edge_splitter(bool is_edge_splitter);
+
     void set_capacity(short);
 
     void set_ptc_num(short);
@@ -173,7 +177,7 @@
     uint16_t edges_capacity_ = 0;
     uint8_t num_non_configurable_edges_ = 0;
 
-    int8_t cost_index_ = -1;
+    int32_t cost_index_ = -1;
     int16_t rc_index_ = -1;
 
     int16_t xlow_ = -1;
@@ -181,6 +185,8 @@
     int16_t xhigh_ = -1;
     int16_t yhigh_ = -1;
 
+    bool is_edge_splitter_ = false;
+
     t_rr_type type_ = NUM_RR_TYPES;
     union {
         e_direction direction; //Valid only for CHANX/CHANY
diff -ruN '--exclude=.git*' '--exclude=vtr_flow' vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/segment_stats.cpp vtr8_avalanche/vpr/src/route/segment_stats.cpp
--- vanilla_vpr/vtr-verilog-to-routing-8.0.0/vpr/src/route/segment_stats.cpp	2020-03-24 17:02:30.000000000 +0100
+++ vtr8_avalanche/vpr/src/route/segment_stats.cpp	2021-08-11 09:16:51.502347000 +0200
@@ -8,6 +8,8 @@
 #include "globals.h"
 #include "segment_stats.h"
 
+#include "route_common.h"
+
 /*************** Variables and defines local to this module ****************/
 
 #define LONGLINE 0
@@ -61,6 +63,8 @@
         }
     }
 
+    print_edge_splitter_costs();
+
     VTR_LOG("\n");
     VTR_LOG("Segment usage by type (index): type utilization\n");
     VTR_LOG("                               ---- -----------\n");
